# Regression Analysis and Model Validation

## A. Data wrangling
Data wrangling is an important step in data analysis. This includes gathering, inspecting, transforming and mapping the data into a appropriate format for a variety of data analyses.

### a.1 Demo data sets

First step is to laod the data to the console. The data set that will be using for can be accessed using this link 'http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt'. Information about the data set can be acessed through this link: 'https://www.mv.helsinki.fi/home/kvehkala/JYTmooc/JYTOPKYS3-meta.txt'.

After loading the data we can check the dimension and structure of the data using **dim()** and **str()**, respectively.

```{r}

date()

# installing packages

#install.packages(c('dplyr', 'ggplot2'))
library(dplyr)
library(GGally)
library(ggplot2)

# Read the data
data <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep = "\t", header = TRUE)

# Inspect the dimension and structure of the data
dim(data)
str(data)


```

### a.2 Adding new columns

It is also possible to add new columns in the data set. In this exercise, we are going to create new column **attitude**, by scaling the column **data$attitude** by 10.

we are also adding new column (**attitude**, **deep**, **stra**, **surf**) corresponding to the mean value for questions related to deep, strategic and surface learning.


```{r}

#-- create 'attitude' column
data$attitude<-data$Attitude/10

#-- create 'deep' column for questions related to deep learning
deep_q<-c("D03", "D11", "D19", "D27", "D07", "D14", 
          "D22", "D30","D06",  "D15", "D23", "D31")
deep_c<-select(data, one_of(deep_q))
data$deep<-rowMeans(deep_c)

#-- create 'stra' column for questions related to strategic learning
strategic_q <- c("ST01","ST09","ST17","ST25","ST04","ST12",
                         "ST20","ST28")

strategic_c<-select(data, one_of(strategic_q))
data$stra<-rowMeans(strategic_c)

#create 'surf' column for questions related to surface learning
surface_q <- c("SU02","SU10","SU18","SU26", "SU05","SU13",
                       "SU21","SU29","SU08","SU16","SU24","SU32")
surface_c<-select(data, one_of(surface_q))
data$surf<-rowMeans(surface_c)

```


### a.2 Subsetting data set

We can create a subset from the original data using **dplyr** package. We are going to creat a data set that only contains a subset of the originial data set and we are calling it **l2014**.

```{r}
library(dplyr)

#-- columns to keep
keep_columns <- c("gender","Age","attitude", "deep", 
                  "stra", "surf", "Points")

#-- create a new dataset
l2014<-select(data, one_of(keep_columns))

#-- check structure
str(l2014)

```

### a.3 Data visualization

We are using **GGally** and **ggplot2** packages in creating a graphical overview of the data set we just created.

```{r}

library(GGally)
library(ggplot2)

#creating plot matrix using ggpairs

p <- ggpairs(l2014, mapping = aes(col = gender, alpha = 0.3),
             lower = list(combo = wrap("facethist", bins = 20)))

p

```

The graphical output is a scatter plot matrix among the parameters with colors corresponding to **gender**. There is gender biased in the data set, however the distribution is generally normal for both gender in all student's attitude towards statistics and approaches to learning. In addition, age distribution is skewed towards younger participants between ages 20 - 30.

In general, there is low linear correlation among the parameters, with the highest correlation found between attitude and exam points (r^{2} = 0.44, F-stat = 0.042).


## B. Multiple Linear Regression

Linear regression is a statistical approach that demonstrates relationships between a dependent variable **y** and one or more explanatory variables **X**.

In this exercise, we are selecting three explanatory variables (attitude, stra, surf) from l2014 data set we created from the previous section to explain the exam points of the students.

```{r eval = FALSE}

#-- visualize the relationships
library(GGally)
library(ggplot2)

ggpairs(l2014, lower = list(combo = wrap("facethist", bins = 20)))

#-- create a regression model with multiple explanatory variables
# lm <- the type of model we are using (linear model)
# points <- dependent variable
# attitude + stra + surf <- explanatory variables

m1 <- lm(points ~ attitude + stra + surf, data = l2014)

#-- print out summary
summary(m1)

#Call:
#lm(formula = points ~ attitude + stra + surf, data = learning2014)
#
#Residuals:
#     Min       1Q   Median       3Q      Max 
#-17.1550  -3.4346   0.5156   3.6401  10.8952 
#
#Coefficients:
#            Estimate Std. Error t value Pr(>|t|)    
#(Intercept)  11.0171     3.6837   2.991  0.00322 ** 
#attitude      3.3952     0.5741   5.913 1.93e-08 ***
#stra          0.8531     0.5416   1.575  0.11716    
#surf         -0.5861     0.8014  -0.731  0.46563    
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
#Residual standard error: 5.296 on 162 degrees of freedom
#Multiple R-squared:  0.2074,	Adjusted R-squared:  0.1927 
#F-statistic: 14.13 on 3 and 162 DF,  p-value: 3.156e-08

```

Summary of the results from the first model (**m1**) shows that student's **attitude** have significant statistical effect to their total exam points.

We can make another model, this time only including **attitude** as the explanatory variable of exam points.

```{r eval = FALSE}
#-- model 2 with only 1 explanatory variabe
m2<-lm(points~attitude, data = l2014)

#-- printing summary
print(summary(m2))

#Call:
#lm(formula = points ~ attitude, data = l2014)
#
#Residuals:
#     Min       1Q   Median       3Q      Max 
#-16.9763  -3.2119   0.4339   4.1534  10.6645 
#
#Coefficients:
#            Estimate Std. Error t value Pr(>|t|)    
#(Intercept)  11.6372     1.8303   6.358 1.95e-09 ***
#attitude      3.5255     0.5674   6.214 4.12e-09 ***
#---
#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
#Residual standard error: 5.32 on 164 degrees of freedom
#Multiple R-squared:  0.1906,	Adjusted R-squared:  0.1856 
#F-statistic: 38.61 on 1 and 164 DF,  p-value: 4.119e-09


```

Model 2 predicts that if **attitude** increased by 1 unit, the exam **points** will increased by as much as **3.5 units**.

However, model needs to be evaluated and check if it violates assumptions for linear regression.

## b.2 Model validation

Linear regression has several assumptions:  
     + errors are normally distributed  
     + errors are not correlated  
     + errors have constant variance  
     + the size of a given error does not depend on the explanatory variable
     
This assumptions can be visually inspected using **plot()** command.

```{r eval = FALSE}

# create a regression model with multiple explanatory variables
l2014<-as.data.frame(l2014)
m3=lm(points ~ attitude + stra, data = l2014)
# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2))
plot(m3, which = c(1, 2, 5))
```

The **Residuals vs Fitted** plot shows no apparent pattern in the residuals suggesting constant variance of the errors. The **Normal Q-Q plot** suggests that the residual is normally distributed. The **Residual vs Leverage plot** suggests no outlier affecting the results. 

